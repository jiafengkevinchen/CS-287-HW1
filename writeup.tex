
\documentclass[11pt]{article}

\usepackage{common}
\usepackage{macros}
\usepackage{nameref}

\title{HW1: Classification}
\author{Jiafeng Chen \and Yufeng Ling \and Francisco Rivera}
\begin{document}

\maketitle{}
\section{Introduction}

This note lays out our expectation for a homework submission in
\textit{CS287: Statistical Natural Language Processing}. While you do
not have to follow this template to the letter, we do expect that
write-ups have a very clear structure and cover all the elements
described in this note. With this in mind, the burden is on the
presenter to demonstrate why deviations from the standard are
necessary.

All write-ups should include a short introduction. In this section you
should summarize the underlying problem in high-level language and
describe the extensions that you have decided to propose in your
implementation. When you describe these extensions you should
carefully cite the papers of interest. For instance, it will often be
useful to cite the work seen in class
\citep{murphy2012machine}. Alternatively, you can also cite papers
inline, for instance the work of \citet{berger1996maximum}.


\section{Problem Description---Old}

In general, homeworks will be specified using informal
language. As part of the assignment, we expect you to write-out a
definition of the problem and your model in formal language. For this
class, we will use the following notation:

\begin{itemize}
\item $\boldb, \boldm$;  bold letters for vectors.
\item $\boldB, \boldM$;  bold capital letters for matrices.
\item $\mcB, \mcM$;  script-case for sets.
\item $b_i, x_i$; lower case for scalars or indexing into vectors.
\end{itemize}


For instance in natural language processing, it is common to use
discrete sets like $\mcV$ for the vocabulary of the language, or $\mcT$ for a
tag set of the language.  We might also want one-hot vectors
representing words. These will be of the type
$\boldv \in \{0,1\}^{|\mcV|}$. In a note, it is crucial to define the
types of all variables that are introduced. The problem description is the
right place to do this.

% NLP is also
% full of sequences. For instance sentences, $w_1, \ldots, w_N$, where
% here $N$ is a constant length and $w_i \in \mcV$ for all
% $i \in \{1, \ldots N\}$. If we pretend sentences are all the same
% length, we can have scoring function over sentences,
% $s : \mcV^N \mapsto \reals$.  One might be defined as:

% \[ s(w_1, \ldots, w_N) = \sum_{i = 1}^N p(w_i | w_{i-2}, w_{i-1}), \]

% \noindent where $p$ is the bigram probability, which we will cover later in
% the class.

\section{Problem Description}

The focus of the problem set is sentiment classification. We are given
\emph{sentences} and aim to classify them as either positive or negative
sentiment (a binary classification). These predictions can be made in a
probabilistic fashion by writing $y_i$ as the event that the $i$th sentence is
positive sentiment, and calculating $p(y_i)$.

Sentences are themselves sequences of words $w \in \mcV$ in some vocabulary
$\mcV$. In particular, there will be two special words, $w_\text{unk},\,
w_\text{pad} \in \mcV$ which represent an unknown word and a padding unit
respectively; we address their significance later. A particular sequence of
words $w_1, \ldots, w_n$ need not have a specific length $n$, which varies
across sentences.

We represent words in two main ways. The first is as a one-hot encoded vector
$\boldv \in \{0,1\}^{|\mcV|}$. This representation is useful for the
\nameref{subsec:logistic} model. Alternatively, we can use a dense embedding.
That is, each word gets assigned a vector $\boldv \in \mathbb{R}^d$ where $d$ is
the embedding dimension. We use \citet{mikolov2013efficient}'s pre-trained word
embeddings ($d=300$) for the \nameref{subsec:cbow} and \nameref{subsec:convnet}
models.


\section{Model and Algorithms}
\label{sec:models}

\subsection{Naive Bayes}
\label{subsec:nb}

As we alluded to earlier, we can treat our prediction problem as probabilistic. We are interested in the probability of
\[
  p(y_i \mid \bm w) = \frac{p(\bm w \mid y_i) p(y_i)}{p(\bm w)}
\]
which is given by Bayes' rule. The Bayesian approach is formulated as follows:
\newcommand{\bdp}{\mathbf{p}}
\newcommand{\bdq}{\mathbf{q}}
\begin{itemize}
  \item We first assign pseudo-counts to each word that act as priors in the Bayesian framework. This results in $\bdp, \bdq \in \mathbb{R}^{|\mcV|}$, representing the counts of words in positive/negative sentences. In our model, we initialized both to be vector of ones.
  \item We train the model by updating $\bdp$ and $\bdq$. We increment $p_i$ (resp. $q_i$) by the occurences of word $w_i$ in positive (resp. negative) sentences.
\end{itemize}
The predicted probabilities for a sentence with features $\bm x_i$ is given by softmaxing over
\[
  \begin{bmatrix}
    \log \left( \frac{\bdp}{\|\bdp\|_1} \right)^T \bm x_i + \log \left( \frac{N_+}{N} \right)\\
    \log \left( \frac{\bdq}{\|\bdq\|_1} \right)^T \bm x_i + \log \left( \frac{N_-}{N} \right)
  \end{bmatrix}
\]
where $N_+$ and $N_-$ are respectively the number of positive sentences and negative sentences in the training set and $N = N_+ + N_-$. In addition, $\bm x_i$ is the max-over-time pooling of one-hot encoding matrix.

\subsection{Logistic Regression}
\label{subsec:logistic}

\subsection{Continuous Bag of Words}
\label{subsec:cbow}

\subsection{Convolutional Neural Network}
\label{subsec:convnet}

\section{Model and Algorithms---Old}

Here you specify the model itself. This section should formally
describe the model used to solve the task proposed in the previous
section. This section should try to avoid introducing new vocabulary
or notation, when possible use the notation from the previous section.
Feel free to use the notation from class, but try to make the note
understandable as a standalone piece of text.

This section is also a great place to include other material that
describes the underlying structure and choices of your model, for
instance here are some example tables and algorithms from full
research papers:



\begin{itemize}
\item diagrams of your model,

  \begin{center}
    \includegraphics[width=0.4\textwidth]{network}
  \end{center}
\item feature tables,

  \begin{center}
    \begin{tabular}{@{}lll@{}}
      \toprule
      &\multicolumn{2}{c}{Mention Features  } \\
      & Feature & Value Set\\
      \midrule
      & Mention Head & $\mcV$ \\
      & Mention First Word & $\mcV$ \\
      & Mention Last Word & $\mcV$ \\
      & Word Preceding Mention & $\mcV$ \\
      & Word Following Mention & $\mcV$\\
      & \# Words in Mention & $\{1, 2, \ldots \}$ \\
      & Mention Type & $\mathcal{T}$ \\
      \bottomrule
    \end{tabular}
  \end{center}

\item pseudo-code,

  \begin{algorithmic}[1]
    \Procedure{Linearize}{$x_1\ldots x_N$, $K$, $g$}
    \State{$B_0 \gets \langle (\langle \rangle, \{1, \ldots, N\}, 0, \boldh_0, \mathbf{0})  \rangle$}
    \For{$m = 0, \ldots, M-1$ }
    \For{$k = 1, \ldots, |B_m|$}
    \For{$i \in \mcR$}
    \State{$(y, \mcR, s, \boldh) \gets \mathrm{copy}(B_m^{(k)})$}
    \For{word $w$ in phrase $x_i$}
    \State{$y \gets y $ append $w$ }
    \State{$s \gets s + \log q(w, \boldh) $ }
    \State{$\boldh \gets \delta(w, \boldh)$}
    \EndFor{}
    \State{$B_{m+|w_i|} \gets B_{m+|w_i|} + (y, \mcR - i, s,   \boldh)$}
    \State{keep top-$K$ of $B_{m+|w_i|}$ by $f(x, y) + g(\mcR)$}
    \EndFor{}
    \EndFor{}
    \EndFor{}
    \State{\Return{$B_{M}^{(k)}$}}
    \EndProcedure{}
  \end{algorithmic}

\end{itemize}


\section{Experiments}

Finally we end with the experimental section. Each assignment will make clear
the main experiments and baselines that you should run. For these experiments
you should present a main results table. Here we give a sample
Table~\ref{tab:results}. In addition to these results you should describe in
words what the table shows and the relative performance of the models.

Besides the main results we will also ask you to present other results
comparing particular aspects of the models. For instance, for word
embedding experiments, we may ask you to show a chart of the projected
word vectors. This experiment will lead to something like
Figure~\ref{fig:clusters}. This should also be described within the
body of the text itself.


\begin{table}[h]
\centering
\begin{tabular}{llr}
 \toprule
 Model &  & Acc. \\
 \midrule
 \textsc{Baseline 1} & & 0.45\\
 \textsc{Baseline 2} & & 2.59 \\
 \textsc{Model 1} & & 10.59  \\
 \textsc{Model 2} & &13.42 \\
 \textsc{Model 3} & & 7.49\\
 \bottomrule
\end{tabular}
\caption{\label{tab:results} Table with the main results.}
\end{table}


\begin{figure}
  \centering
  \includegraphics[width=6cm]{cluster_viz}
  \caption{\label{fig:clusters} Sample qualitative chart.}
\end{figure}


\section{Conclusion}

End the write-up with a very short recap of the main experiments and the main
results. Describe any challenges you may have faced, and what could have been
improved in the model.

\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
